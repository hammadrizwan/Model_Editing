{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from models import SiameseClassificationNetwork,SiameseNetwork\n",
    "import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hrk21/miniconda3/envs/modelediting/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5Model\n",
    "import nethook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/home/hrk21/miniconda3/envs/modelediting/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "def get_model(PATH: str,device):\n",
    "\n",
    "    # Model and tokenizer\n",
    "    # model_name = \"google/t5-small-ssm-nq\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(PATH, local_files_only=True)\n",
    "    model = T5Model.from_pretrained(PATH, local_files_only=True)\n",
    "    model.to(device)\n",
    "    return model,tokenizer\n",
    "model_path=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/models--google--t5-small-ssm-nq/snapshots/4371c64b6f65176f6663af43066bd094597b1116\"\n",
    "model_t5,tokenizer=get_model(model_path,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=SiameseNetwork(512).to(device)\n",
    "path_to_folder=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/Projector_Networks/Contrastive_Learning_Automated_AVG_EMB/contrastive_mode_500_0/best_model_weights.pth\"\n",
    "state_dict = torch.load(path_to_folder,map_location=torch.device('cpu'))\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Loading data completed\n",
      "Data Processing\n"
     ]
    }
   ],
   "source": [
    "def read_dataset_reduced(file_path_read_dataset: str,data_size):\n",
    "    dataset=[]\n",
    "    values_list = list(range(1, data_size+1))\n",
    "    for number in values_list:\n",
    "        \n",
    "        # print(json.loads(linecache.getline(file_path_read_dataset, number)))\n",
    "        # print(linecache.getline(file_path_read_dataset, number).strip())\n",
    "        data_entry = json.loads(linecache.getline(file_path_read_dataset, number).strip())\n",
    "        dataset.append(data_entry)\n",
    "    return dataset\n",
    "num_samples=10\n",
    "# file_path_dataset=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/counterface_dataset_avg_embedding.jsonl\"\n",
    "# file_path_dataset=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/counterface_dataset_special_embedding.jsonl\"\n",
    "file_path_dataset=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/counterface_dataset_special_embedding_reduced_500_openai_paraphrases_updated_final_processed.jsonl\"\n",
    "# file_path_datase=\"/home/hrk21/projects/def-hsajjad/hrk21/datasets/counterface_dataset_avg_embedding.jsonl\"\n",
    "control=0#version of datasplit to be used, 0 based on high sim, 1 on low sim and 3 on random \n",
    "label_reversal=False# no longer required cost functions updated\n",
    "loss=\"contrastive\" # cosine, cosine_crossentropy, contrastive\n",
    "print(\"Loading data\")\n",
    "# dataset=read_dataset(file_path_dataset,data_size=num_samples)\n",
    "dataset=read_dataset_reduced(file_path_dataset,data_size=num_samples) \n",
    "print(\"Loading data completed\")\n",
    "# dp.create_dataset_pairs(\n",
    "# dataset_paired_train,dataset_paired_test=    create_dataset_pairs_projection(dataset,projector,neightbour_control=0,label_reversal=False)\n",
    "print(\"Data Processing\")\n",
    "dataset_paired_train,dataset_paired_test=dp.create_dataset_pairs(dataset,control,label_reversal)#neighbourhood selection type 0 and reverse labels for Constrastive\n",
    "# dataset_paired_train_openai=dp.data_construct_high_sim(dataset_paired_train,neightbour_control=0,label_reversal=label_reversal,comparison=\"sim\",topk_neg=15,topk_pos=0)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[\"encoder.block.4.layer.1.DenseReluDense.wo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Google Patents, a product developed by'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_paired_train[38][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"Delta Goodrem, that was formulated in\"\n",
    "# sent2=\"Delta Goodrem, that originated in\"\n",
    "sent2=\"Delta Goodrem, a product of\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"orignal_prompt\": [\"Delta Goodrem, that originated in\", \"Australia\"], \"edited_prompt\": [\"Delta Goodrem, that originated in\", \"India\"], \"edited_prompt_paraphrases_processed\": \"Delta Goodrem was from\", \"edited_prompt_paraphrases_unprocessed\": \"The first president of the team was doctor Hector Priem. Delta Goodrem was from\", \"edited_prompt_paraphrases_processed_testing\": \"Delta Goodrem, that was formulated in\", \"edited_prompt_paraphrases_unprocessed_testing\": \"Then, Yuu was supposed to take care of him. Delta Goodrem, that was formulated in\", \"neighborhood_prompts_high_sim\": [\"Fortress, that originated in\", \"K-9 originated in\", \"Fortress, that was created in\", \"Sanctum was developed in\", \"The Libertine, created in\"], \"neighborhood_prompts_low_sim\": [\"Don't Say a Word, that was formulated in\", \"The Pacific was created in the country of\", \"Strange Bedfellows was created in\", \"Cut Copy, formulated in\", \"Until the End of the World, developed in\"], \"vector_edited_prompt\": \"\", \"openai_usable_paraphrases\": [\"Delta Goodrem, coming from\", \"Delta Goodrem, with its roots in\", \"The birthplace of Delta Goodrem was\", \"The starting point for Delta Goodrem was\", \"Delta Goodrem, having its origins in\", \"Delta Goodrem, hailing from\", \"Delta Goodrem, a product of\", \"The source of Delta Goodrem was\", \"Delta Goodrem, which can be traced back to\", \"Delta Goodrem, that traces its origins back to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_usable_paraphrases=[\"Delta Goodrem, coming from\", \"Delta Goodrem, with its roots in\", \"The birthplace of Delta Goodrem was\", \"The starting point for Delta Goodrem was\", \"Delta Goodrem, having its origins in\", \"Delta Goodrem, hailing from\", \"Delta Goodrem, a product of\", \"The source of Delta Goodrem was\", \"Delta Goodrem, which can be traced back to\", \"Delta Goodrem, that traces its origins back to\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id=8\n",
    "# sent2=dataset_paired_train[id][5]\n",
    "# print(dataset_paired_train[id][5])\n",
    "# #\"Angola is a part of the continent of\"#\"Johann Wolfgang von Goethe, who has a citizenship from\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent1 Delta Goodrem, that was formulated in\n",
      "sent2 Delta Goodrem, coming from\n",
      "51.218441009521484\n",
      "sent2 Delta Goodrem, with its roots in\n",
      "40.691802978515625\n",
      "sent2 The birthplace of Delta Goodrem was\n",
      "44.992855072021484\n",
      "sent2 The starting point for Delta Goodrem was\n",
      "73.1138687133789\n",
      "sent2 Delta Goodrem, having its origins in\n",
      "42.393470764160156\n",
      "sent2 Delta Goodrem, hailing from\n",
      "77.70214080810547\n",
      "sent2 Delta Goodrem, a product of\n",
      "24.108654022216797\n",
      "sent2 The source of Delta Goodrem was\n",
      "37.90758514404297\n",
      "sent2 Delta Goodrem, which can be traced back to\n",
      "41.31798553466797\n",
      "sent2 Delta Goodrem, that traces its origins back to\n",
      "28.05472755432129\n"
     ]
    }
   ],
   "source": [
    "#special token\n",
    "print(\"Sent1\",sent1)\n",
    "for sent2 in openai_usable_paraphrases:\n",
    "\n",
    "    with nethook.TraceDict(model_t5, l) as ret:\n",
    "        inputs=tokenizer(sent1, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        outputs = model_t5(inputs, decoder_input_ids=inputs, output_hidden_states=True)\n",
    "        v1=torch.tensor([[ret[layer_fc1_vals].output[:,-1, :] for layer_fc1_vals in ret][0][0].detach().cpu().numpy().tolist()])\n",
    "\n",
    "        inputs=tokenizer(sent2, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "        outputs = model_t5(inputs, decoder_input_ids=inputs, output_hidden_states=True)\n",
    "        v2=torch.tensor([[ret[layer_fc1_vals].output[:,-1, :] for layer_fc1_vals in ret][0][0].detach().cpu().numpy().tolist()])\n",
    "        output1, output2 = model(v1,v2)\n",
    "        print(\"sent2\",sent2)\n",
    "        print(F.pairwise_distance(output1, output2, keepdim=True)[0][0].item())\n",
    "        # print(F.pairwise_distance(v1, v2, keepdim=True)[0][0].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.108654022216797\n",
      "56.38518524169922\n"
     ]
    }
   ],
   "source": [
    "output1, output2 = model(v1,v2)\n",
    "print(F.pairwise_distance(output1, output2, keepdim=True)[0][0].item())\n",
    "print(F.pairwise_distance(v1, v2, keepdim=True)[0][0].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #avg token\n",
    "# with nethook.TraceDict(model_t5, l) as ret:\n",
    "#     inputs=tokenizer(sent1, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "#     outputs = model_t5(inputs, decoder_input_ids=inputs, output_hidden_states=True)\n",
    "#     v1=torch.tensor([[ret[layer_fc1_vals].output[:,:-1, :].squeeze(0).mean(dim=0).detach().cpu().numpy().tolist() for layer_fc1_vals in ret][0]])\n",
    "\n",
    "#     inputs=tokenizer(sent2, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "#     outputs = model_t5(inputs, decoder_input_ids=inputs, output_hidden_states=True)\n",
    "#     v2=torch.tensor([[ret[layer_fc1_vals].output[:,:-1, :].squeeze(0).mean(dim=0).detach().cpu().numpy().tolist() for layer_fc1_vals in ret][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8994]], grad_fn=<MmBackward0>)\n",
      "tensor([[0.5724]])\n"
     ]
    }
   ],
   "source": [
    "output1, output2 = model(v1,v2)\n",
    "print(util.cos_sim(output1, output2))\n",
    "print(util.cos_sim(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_paired_test[0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_paired_test[0][1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelediting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
